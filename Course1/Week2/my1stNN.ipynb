{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessed_mnist import load_dataset\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
    "print(X_train.shape, y_train.shape)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(X_train[0], cmap=\"Greys\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build simple NN with 2 hidden layer\n",
    "# w1.shape=(784, 256) \n",
    "# b1.shape=(256,) \n",
    "# w2.shape=(256, 10)\n",
    "# b2.shape=(10,)\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# define variables\n",
    "feature_num = X_train.shape[1] * X_train.shape[2]\n",
    "digit_num = 10\n",
    "middle_neuron_num = 512\n",
    "input_x = tf.placeholder(tf.float32, shape=(None, X_train.shape[1], X_train.shape[2]), name='input_x')\n",
    "input_x_flatten = tf.reshape(input_x, [-1, feature_num], name='input_x_flatten')\n",
    "input_y = tf.placeholder(tf.uint8, shape=(None,), name='input_y')\n",
    "input_y_one_hot = tf.one_hot(input_y, digit_num, name = 'input_y_one_hot') \n",
    "\n",
    "w1 = tf.Variable(np.zeros((feature_num, middle_neuron_num), dtype='float32'), name='w1')\n",
    "b1 = tf.Variable(np.zeros((middle_neuron_num,), dtype='float32'), name='b1')\n",
    "w2 = tf.Variable(np.random.randn(middle_neuron_num, digit_num) * 0.01, dtype='float32', name='w2')\n",
    "b2 = tf.Variable(np.zeros((digit_num,), dtype='float32'), name='b2')\n",
    "\n",
    "# construct NN, use cross-entropy to compute loss function\n",
    "a1 = tf.nn.sigmoid(tf.matmul(input_x_flatten, w1)+b1, name='a1')\n",
    "a2 = tf.matmul(a1, w2) + b2\n",
    "predicted_y_distribution = tf.nn.softmax(a2, name='predicted_y_distribution')\n",
    "loss = tf.reduce_mean(tf.reduce_sum(-input_y_one_hot*tf.log(predicted_y_distribution), 1), name='loss')\n",
    "predicted_y = tf.cast(tf.argmax(predicted_y_distribution, axis=1), tf.uint8, name='predicted_y')\n",
    "correct_prediction = tf.equal(predicted_y, input_y, name='correct_prediction')\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "\n",
    "# training\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.004).minimize(loss, var_list=(w1, b1, w2, b2))\n",
    "s = tf.InteractiveSession()\n",
    "s.run(tf.global_variables_initializer())\n",
    "tf.train.start_queue_runners()\n",
    "batch_size = 250\n",
    "for i in range(4001):\n",
    "    start_index = i*batch_size % X_train.shape[0]\n",
    "    end_index = start_index + batch_size\n",
    "    if start_index == 0: # shuffle training data\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "    x_batch = X_train[start_index:end_index]\n",
    "    y_batch = y_train[start_index:end_index]\n",
    "    s.run(optimizer, {input_x: x_batch, input_y: y_batch})\n",
    "    if i % 100 ==0:\n",
    "        loss_i = s.run(loss, {input_x: X_train, input_y: y_train})\n",
    "        accuracy_i = s.run(accuracy, {input_x: X_test, input_y: y_test})\n",
    "        print(\"iter %i: loss=%.4f, accuracy on test=%.4f\" % (i, loss_i, accuracy_i))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
